<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Churn Prediction | Tetyana Nesdill</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Classification Problem">
    <meta name="generator" content="Hugo 0.82.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    

  
  
    <link rel="stylesheet" href="https://tsolopchuk.github.io/Tetyana_Portfolio/ananke/dist/main.css_5c99d70a7725bacd4c701e995b969fea.css" >
  




    
      

    

    
    
    <meta property="og:title" content="Churn Prediction" />
<meta property="og:description" content="Classification Problem" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tsolopchuk.github.io/Tetyana_Portfolio/post/chapter-6/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2020-10-14T11:25:05-04:00" />
<meta property="article:modified_time" content="2020-10-14T11:25:05-04:00" /><meta property="og:site_name" content="Tetyana Nesdill" />

<meta itemprop="name" content="Churn Prediction">
<meta itemprop="description" content="Classification Problem"><meta itemprop="datePublished" content="2020-10-14T11:25:05-04:00" />
<meta itemprop="dateModified" content="2020-10-14T11:25:05-04:00" />
<meta itemprop="wordCount" content="2101">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Churn Prediction"/>
<meta name="twitter:description" content="Classification Problem"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://tsolopchuk.github.io/Tetyana_Portfolio/images/onur-binay-mVcTLcRAknM-unsplash.jpg');">
    <div class="pb2-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://tsolopchuk.github.io/Tetyana_Portfolio/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Tetyana Nesdill
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://tsolopchuk.github.io/Tetyana_Portfolio/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://tsolopchuk.github.io/Tetyana_Portfolio/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://tsolopchuk.github.io/Tetyana_Portfolio/post/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      















    </div>
  </div>
</nav>

      <div class="tc-l pv5 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Churn Prediction</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Classification Problem
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      <h1 class="f1 athelas mt3 mb1">Churn Prediction</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-10-14T11:25:05-04:00">October 14, 2020</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>Link to GitHub project: <a href="https://github.com/tsolopchuk/churn">https://github.com/tsolopchuk/churn</a></p>
<p>Telecom industry is highly saturated when it comes to new customer acquisition. Everyone has a phone or two and there is not much room left for growth of market share for the telecom providers. To grow the market share telecom providers either a) need to convince people they need even more connected devices per household or b) win market share from the competitors. Wining competitors’ market share is important; however, telecom companies also need to make sure that they keep their base or current customers satisfied with the service and reduce churn among their customer base. This analysis is focused on predicting churn using supervised machine learning.</p>
<p><strong>Objectives:</strong></p>
<p>(1) Create supervised machine learning algorithms to predict customer churn in telecom industry;</p>
<p>(2) Evaluate several developed classification models and select a model with the most accurate prediction.</p>
<p><strong>As in any modeling exercise a 5-step approach can be used:</strong></p>
<ol>
<li>Collect data</li>
<li>Explore and prepare the dataset for analysis</li>
<li>Train model on the dataset</li>
<li>Evaluate model performance</li>
<li>Improve the model</li>
</ol>
<p><strong>(1)	Collect Data</strong></p>
<p>The dataset used is a sample dataset obtained from Kaggle.  The goal is to explore mechanics of supervised machine learning and model evaluation.</p>
<p>The latest version of the data can be found here <a href="https://www.kaggle.com/blastchar/telco-customer-churn">https://www.kaggle.com/blastchar/telco-customer-churn</a>. In this post I am using one of the previous versions of this dataset.</p>
<p><strong>(2)	Explore and Prepare the Dataset for Analysis</strong></p>
<p>The dataset includes 7,043 observations and 9 variables. One of the 9 variables is customer ID, which will be excluded from the classification analysis as it does not add any valuable information for customer classification. Churn is the class that will be predicted. These leaves 7 variables that are going to be used to train the model and predict customer churn.</p>
<p>There are 3 continuous variables and 4 categorical variables. Let’s take a closer look at our continuous variables: tenure, monthly charges and total charges. Below is a plot of the continuous variables, where different colors represent customer churn. It seems that customers with high monthly charges and low tenure have higher churn (red points).</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/Scatterplot.png"/> 
</figure>

<p>Now, let’s take a look at 4 categorical variables: contract, phone service, payment method and paperless billing. It can be clearly seen that customers that are on month-to-month plans have higher churn. In month-to-month segment there is about 45% churn rate. While some other indicators such as phone service (or landline access) have about 25% of churn for both customers with landline access and without.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/Categorical%20var.png"/> 
</figure>

<p><strong>(3.A) Train model on the dataset – KNN</strong></p>
<p>The first model used to predict customer churn based on the 7 variables is KNN. KNN is a lazy classifier and it really does not want to be trained. However, it does want couple of things: a) all categorical variables must be numeric; b) all continuous variables must be normalized.</p>
<p>To prepare the categorical variables for KNN prediction, they must be converted to numeric values. For example, if churn was “yes”, assign 1, if churn is “no”, assign 0.</p>
<p>It is important to normalize the continuous variables to make sure that all of the three variables have about equal impact on the overall prediction. As per below screenshot the tenure variable falls between 0 and 72 months, monthly charges have a range of 18.25 to 118.75 and the total charges range values fall between 18.8 and 8684.8. this means that the total charges variable is going to have significantly higher impact in the distance calculation. Normalization process includes subtracting from each value in x the min value of x and divide by the range of x.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/cont%20vars%20summary_2.png"/> 
</figure>

<p>It also appears that Total Charges variable has 11 missing values. R mice package is used to impute mean values to replace NAs.</p>
<p>After normalizing the continuous variables, they have the below values.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/cont%20vars%20new_2.png"/> 
</figure>

<p>The dataset is then split into training subset and test subset. Training subset includes 4,403 observations, and test dataset includes 2,640 observations.</p>
<p>For the first run the value is  k=66, which is a square root of 4,403 (number of observations in the training data subset).</p>
<p><strong>(4.A) Evaluate model performance - KNN</strong></p>
<p>To evaluate model performance, let’s take a look at the confusion matrix. 166 observations or 6.3% were incorrectly classified as churn. 360 observations were incorrectly classified as no churn, while in fact customers left the company. The overall accuracy of the model is 80.1%. This means that in 80.1% of cases the churn class label was correctly predicted.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/KNN%20results_2.png"/> 
</figure>

<p><strong>(5.A) Improve model performance – KNN</strong></p>
<p>In order to try to improve model performance, KNN model is re-run with different values of k. Initially k=66 was selected, which is a square root of the number of observations in the test dataset. The results for the new model runs are included below.</p>
<table>
<thead>
<tr>
<th>Run</th>
<th>#	K</th>
<th>False Positive</th>
<th>False Negative</th>
<th>Error Rate</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>66</td>
<td>0.065</td>
<td>0.137</td>
<td>0.202</td>
<td>0.798</td>
</tr>
<tr>
<td>2</td>
<td>50</td>
<td>0.066</td>
<td>0.136</td>
<td>0.202</td>
<td>0.798</td>
</tr>
<tr>
<td>3</td>
<td>70</td>
<td>0.066</td>
<td>0.139</td>
<td>0.205</td>
<td>0.795</td>
</tr>
<tr>
<td>4</td>
<td>100</td>
<td>0.055</td>
<td>0.151</td>
<td>0.206</td>
<td>0.794</td>
</tr>
<tr>
<td>5</td>
<td>150</td>
<td>0.055</td>
<td>0.154</td>
<td>0.209</td>
<td>0.791</td>
</tr>
<tr>
<td>6</td>
<td>30</td>
<td>0.081</td>
<td>0.130</td>
<td>0.211</td>
<td>0.789</td>
</tr>
</tbody>
</table>
<p>Unfortunately, in this case there was no significant improvement in model performance by selecting different k-values. The original value of k=66 is kept.</p>
<p><strong>(3.B)  Train model on the dataset – Naïve Bayes</strong></p>
<p>The next model that is used for this classification problem is Naïve Bayes. A few changes are required to the continues variables to run the model. In case of Naïve Bayes the categorical variables do not need to be transformed. However, Naïve Bayes does not accept the numeric variables, so the transformation of continuous variables is required. Below are the boxplots for the continuous variables that will be used to convert continuous variables to categorical.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/Boxplots.png"/> 
</figure>

<p>In order to convert continuous variables to categorical, categories are created and each observation is assigned to a corresponding category. For example, mean tenure is 32.37 (as per above box plots) with relatively normal distribution. All observations where tenure is less than 32.37 are categorized as “newbie” and all observations where tenure is greater than 32.37 are categorized as “tenured”. This way instead of having a continuous variable for tenure, a categorical variable is created with two values: “newbie” or “tenured”. In a similar way Monthly Charges variable is converted to a categorical variable with values of “low” and “high” based on the mean value of 64.76. Given that Total Charges are highly skewed as indicated in the above boxplots, the median value of 1397.5 is used to categorize observations as “low” and “high” as well.
Next, the dataset is divided the same way as for KNN model into training and test subsets. The Naïve Bayes model can be run and used to predict customer churn.</p>
<p><strong>(4.B) Evaluate model performance – Naïve Bayes</strong></p>
<p>Confusion matrix is used to evaluate model performance. There were 313 cases incorrectly classified as no churn and 305 cases incorrectly classified as churn. It is interesting to note that there are fewer False Negatives than if using KNN. If a company values more correctly identifying the churn customers, then Naïve Bayes should be used rather than KNN even though the overall accuracy of the Naïve Bayes model is lower.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/NB_results_2.png"/> 
</figure>

<p><strong>(3.C) Train model on the dataset – Decision Tree</strong></p>
<p>The last model is decision tree. A convenient characteristic about working with decision trees is that categorical or continuous variables can be used and no transformation is needed. The original dataset is simply divided into training and test subsets. It is hard to read the individual “leaves”, but it can be noted that the algorithm divided the data into 18 buckets or “leaves”.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/tree.PNG"/> 
</figure>

<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/Tree%20pic_2.png"/> 
</figure>

<p>Highlighted are the five leaves were more than half of the customers churned.</p>
<ul>
<li>High monthly charges &gt;68.55 anssd tenure less than 16 months in buckets D and E</li>
<li>Tenure less than 5 months and monthly charges &gt;20.85 in buckets B and C</li>
<li>Tenure less than 1 month and monthly charges &lt;20.85 in bucket A</li>
</ul>
<p><strong>(4.C) Evaluate model performance – Decision Tree</strong></p>
<p>Tree model produced 260 false positives and 286 false negatives with the overall accuracy of ~79%.. So far this is the best model in terms of the number of false negatives. The last step is trying to improve the model performance. Again, if a company cares more about correctly predicting customers that churned, the decision tree model is a better choice as only 10.8% of customers were incorrectly classified as no churn.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/Tree%20results_2.png"/> 
</figure>

<p><strong>BRINING IT ALL TOGETHER</strong></p>
<p>To compare three models, the ROC curve is constructed. ROC shows trade off between the detection of true positives (Sensitivity) while avoiding the false positives (1-Specificity). The perfect classifier would have 100% true positive rate and 0% false positive rate.  The more the curve is bent upwards toward the left top corner, the closer the model is to becoming “the perfect classifier”.</p>
<p>On the below chart the ROC is plotted for all three models: KNN – green, Naïve Bayes – red, and Decision Tree – blue. It appears that Naïve Bayes model and Decision Tree model have similar ROC curves. They are both far from the perfect classifier, but this shows clearly the trade off between Sensitivity and 1-Specificity or the trade off between true positive and false positive results.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/ROC.png"/> 
</figure>

<p>Additionally, let’s calculate area under the curve (AUC) for three models. AUC measures the total area under the ROC curve. The higher the AUC, the closer is a classifier to the perfect classifier. The following results are obtained: AUC = 0.8069 for Naïve Bayes; AUC = 0.6875 for KNN, and AUC = 0.8159 for Decision Tree. Naïve Bayes and Decision Tree models have similar AUC, as previously noted their ROC curves are also very close. AUC &gt; 0.8 can be classified as “excellent/good” classifier.</p>
<p>The bellow table brings together all model evaluation criteria discussed.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>KNN</th>
<th>Naïve Bayes</th>
<th>Decision Tree</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td>0.8008</td>
<td>0.7659</td>
<td>0.7932</td>
</tr>
<tr>
<td>Sensitivity</td>
<td>0.4752</td>
<td>0.5437</td>
<td>0.6061</td>
</tr>
<tr>
<td>Specificity</td>
<td>0.9150</td>
<td>0.8439</td>
<td>0.8556</td>
</tr>
<tr>
<td>AUC</td>
<td>0.6875</td>
<td>0.8069</td>
<td>0.8159</td>
</tr>
</tbody>
</table>
<p>Based on the AUC results, KNN is not as strong of a classifier as the other two models. However, KNN gives the highest accuracy. The best way to pick a model in this case is to go back to the original business problem and answer the question: “what are we trying to solve?”</p>
<p>In this example the assumption is that a telecom provider values more proactively identifying customers that churned. In other words, false negatives are more important. Every time a customer leaves telecom provider, the company loses part of its market share to the competitors. If modeling exercise can help identify which group of customers is likely to churn, then such group can be a target for marketing campaigns to try to convince people not to leave their current provider and provide them with additional incentives to stay.
Based on this logic, the sensitivity (true positive rate) is more important rather than overall model accuracy or high specificity (true negative rate). In this scenario the most appropriate model that helps correctly predict churn is the Decision Tree model.</p>
<p><strong>Final tree and conclusion – Immediate action</strong></p>
<p>As previously noted, it does seem that certain variables have higher impact on churn prediction when using decision tree. For example, the contract, monthly charges and tenure have higher impact on the classification than paperless billing enrolment, landline access, etc. The first decision tree explored had 18 leaves, meaning that the data was divided in 18 buckets. It is hard to understand and use for classification purposes. Can the decision model be further simplified to facilitate decision-making?</p>
<p>The decision tree needs to be “pruned’. In the next run only the following variables are included: contract, tenure, and monthly charges. This change divided the observations into 6 buckets, which makes it much easier to interpret the results.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/final%20tree.png"/> 
</figure>

<p>The confusion matrix shows for the simplified tree shows that the sensitivity actually improved. The new sensitivity (or true positive rate) is 68.87%.</p>
<figure>
    <img src="https://tsolopchuk.github.io/Tetyana_Portfolio/images/Pics%20-%20class/final%20tree%20results.png"/> 
</figure>

<p>Sensitivity = True positive / True positive + False negative = 250/ (250 + 113) = 0.6887 *100% = 68.87%.
As a conclusion, customers that pay more than &gt;$68.55 per month and have a tenure less than 16 months are more likely to churn. This is a target segment that needs attention of marketing and sales teams to ensure company continues to grow its customer base rather than loosing existing customers to competition.  This is a high-risk churn group of customers that might benefit from additional incentives and targeted marketing campaigns. 68.87% of churned results were correctly predicted with the overall model accuracy of 79%. In a real-world scenario additional data points can be used to further improve churn modeling, such as number of lines on a plan, data usage, charges for data overages, customer service satisfaction.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://tsolopchuk.github.io/Tetyana_Portfolio/" >
    &copy;  Tetyana Nesdill 2021 
  </a>
    <div>














</div>
  </div>
</footer>

  </body>
</html>
